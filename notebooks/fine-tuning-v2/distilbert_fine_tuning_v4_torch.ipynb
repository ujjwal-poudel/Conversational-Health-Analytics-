{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9384976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ujjwalpoudel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ujjwalpoudel/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AdamW, get_scheduler, AutoConfig\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "\n",
    "# Importing the custom preprocessor\n",
    "sys.path.append(str(Path.cwd().parents[1] / \"src\"))\n",
    "from preprocessing.preprocess_dataset import DistilBertPreprocessor\n",
    "from transformers import (\n",
    "    AutoConfig, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    ")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "425307fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd().parents[2]  # Adjust based on notebook location\n",
    "\n",
    "DATA_DIR = Path(\"/Volumes/MACBACKUP/final_datasets/\")   # The dataset folder\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\" / \"distilbert-v4\"  # Folder to save model checkpoints\n",
    "PLOT_DIR = PROJECT_ROOT / \"plots\" / \"distilbert-v4\"   # Folder to save plots\n",
    "\n",
    "# Create directories if they do not exist\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Adding src folder to sys.path if you have helper modules\n",
    "sys.path.append(str(PROJECT_ROOT / \"src\"))\n",
    "\n",
    "# Model & Training Config\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "LEARNING_RATE = 2e-5      # Good starting point for DistilBERT fine-tuning\n",
    "EPOCHS = 10               # Avoid overly long training\n",
    "BATCH_SIZE = 8\n",
    "EARLY_STOPPING_PATIENCE = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ebe49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPARATION\n",
    "#  PYTORCH DATASET CLASS\n",
    "class DepressionDataset(Dataset):\n",
    "    \"\"\"Custom Dataset class for our depression text data.\"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The encodings are already tensors from the tokenizer\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e69a7eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return avg_loss, acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d78aa8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history, model_name=\"distilbert-v4\", base_dir=\"plots\", show=True):\n",
    "    \"\"\"\n",
    "    Generates, displays, and saves training/validation metric plots.\n",
    "\n",
    "    Args:\n",
    "        history (dict): Contains 'train_loss', 'val_loss', 'train_accuracy',\n",
    "                        'val_accuracy', and 'val_f1'.\n",
    "        model_name (str): Subfolder name under 'plots/' (e.g., \"distilbert-v4\").\n",
    "        base_dir (str or Path): Base directory for saving plots.\n",
    "        show (bool): If True, display plots inline in notebook.\n",
    "    \"\"\"\n",
    "    # Prepare output directory\n",
    "    output_dir = Path(base_dir) / model_name\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    # Plot 1: Loss Curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, history['train_loss'], 'b-o', label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], 'r-o', label='Validation Loss')\n",
    "    plt.title(f'{model_name.upper()} - Training vs Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    loss_path = output_dir / 'training_validation_loss.png'\n",
    "    plt.savefig(loss_path, dpi=300)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 2: Accuracy & F1 Curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, history['train_accuracy'], 'c-o', label='Training Accuracy')\n",
    "    plt.plot(epochs, history['val_accuracy'], 'g-o', label='Validation Accuracy')\n",
    "    plt.plot(epochs, history['val_f1'], 'm-o', label='Validation F1-Score')\n",
    "    plt.title(f'{model_name.upper()} - Accuracy and F1 Trends')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    metrics_path = output_dir / 'accuracy_f1.png'\n",
    "    plt.savefig(metrics_path, dpi=300)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved plots to: {output_dir}\")\n",
    "    print(f\"   - Loss curve: {loss_path}\")\n",
    "    print(f\"   - Accuracy & F1 curve: {metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77d42cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (GPU) for training.\n"
     ]
    }
   ],
   "source": [
    "# Device Setup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (GPU) for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1e58ed",
   "metadata": {},
   "source": [
    "### Data loading and Pre-procesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eac11c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2218 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded /Volumes/MACBACKUP/final_datasets/final_train_dataset.csv. Found 107 records.\n",
      "Text cleaning complete.\n",
      "Data prepared. X shape: (107, 1), y shape: (107, 1)\n",
      "\n",
      "Successfully loaded /Volumes/MACBACKUP/final_datasets/final_dev_dataset.csv. Found 35 records.\n",
      "Text cleaning complete.\n",
      "Data prepared. X shape: (35, 1), y shape: (35, 1)\n",
      "\n",
      "Starting chunking with strategy: 'sentence_aware'...\n",
      "Chunking complete. Original docs: 107, Total chunks: 407\n",
      "Starting chunking with strategy: 'sentence_aware'...\n",
      "Chunking complete. Original docs: 35, Total chunks: 147\n",
      "Tokenizing for DistilBERT with max_length=512...\n",
      "Tokenizing for DistilBERT with max_length=512...\n"
     ]
    }
   ],
   "source": [
    "# Data Loading and Preprocessing\n",
    "preprocessor = DistilBertPreprocessor()\n",
    "\n",
    "# Loading raw data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "X_train_df, y_train_df = preprocessor.load_and_preprocess(DATA_DIR / \"final_train_dataset.csv\")\n",
    "X_dev_df, y_dev_df = preprocessor.load_and_preprocess(DATA_DIR / \"final_dev_dataset.csv\")\n",
    "\n",
    "# Sentence-aware chunking\n",
    "X_train_chunked, y_train_chunked = preprocessor.chunk_dataframe(X_train_df, y_train_df, strategy=\"sentence_aware\", chunk_size=510, overlap=2)\n",
    "X_dev_chunked, y_dev_chunked = preprocessor.chunk_dataframe(X_dev_df, y_dev_df, strategy=\"sentence_aware\", chunk_size=510, overlap=2)\n",
    "\n",
    "# Tokenization\n",
    "train_encodings = preprocessor.tokenize(X_train_chunked['text'])\n",
    "dev_encodings = preprocessor.tokenize(X_dev_chunked['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1712f9",
   "metadata": {},
   "source": [
    "### DATASET AND DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37fd521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = y_train_chunked['label'].tolist()\n",
    "dev_labels = y_dev_chunked['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "465c703c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: tensor([0.5553, 1.4447])\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights\n",
    "label_counts = y_train_chunked['label'].value_counts().sort_index()\n",
    "class_sample_count = torch.tensor(label_counts.values, dtype=torch.float)\n",
    "class_weights = 1. / class_sample_count\n",
    "class_weights = class_weights / class_weights.sum() * 2  # normalizes roughly to [1, 2] range\n",
    "\n",
    "print(f\"Class Weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfc3e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "train_dataset = DepressionDataset(train_encodings, train_labels)\n",
    "dev_dataset = DepressionDataset(dev_encodings, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2f37c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders (NO sampler)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dcf524",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e87dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_distilbert_classifier(num_labels=2):\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=num_labels,\n",
    "        attention_dropout=0.3,\n",
    "        dropout=0.2\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18cf4a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ujjwalpoudel/Documents/insane_projects/Conversational-Health-Analytics-/.newvenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ujjwalpoudel/Documents/insane_projects/Conversational-Health-Analytics-/.newvenv/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model, optimizer = build_distilbert_classifier(num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8170e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduler\n",
    "num_training_steps = EPOCHS * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76217519",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "608856d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': [], 'val_f1': []}\n",
    "best_val_accuracy = 0.0\n",
    "patience_counter = 0\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04977797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/51 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.7008 | Train Acc: 0.6388 | Val Loss: 0.6908 | Val Acc: 0.5986 | Val F1: 0.0000\n",
      "Validation accuracy improved from 0.0000 → 0.5986. Saving model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CHECKPOINT_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     47\u001b[39m best_val_accuracy = val_accuracy\n\u001b[32m     48\u001b[39m patience_counter = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mCHECKPOINT_PATH\u001b[49m.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     50\u001b[39m model.save_pretrained(CHECKPOINT_PATH)\n\u001b[32m     51\u001b[39m preprocessor.tokenizer.save_pretrained(CHECKPOINT_PATH)\n",
      "\u001b[31mNameError\u001b[39m: name 'CHECKPOINT_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch + 1}/{EPOCHS} ---\")\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    epoch_preds, epoch_labels = [], []\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        epoch_preds.extend(preds.cpu().numpy())\n",
    "        epoch_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_accuracy = accuracy_score(epoch_labels, epoch_preds)\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['train_accuracy'].append(train_accuracy)\n",
    "\n",
    "    print(\"Evaluating on validation set...\")\n",
    "    val_loss, val_accuracy, val_f1 = evaluate(model, dev_loader, device, loss_fn)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_accuracy'].append(val_accuracy)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f} \"\n",
    "          f\"| Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        print(f\"Validation accuracy improved from {best_val_accuracy:.4f} → {val_accuracy:.4f}. Saving model...\")\n",
    "        best_val_accuracy = val_accuracy\n",
    "        patience_counter = 0\n",
    "    \n",
    "        # Save the model and tokenizer to your existing folder\n",
    "        model.save_pretrained(MODEL_DIR)\n",
    "        \n",
    "        preprocessor.tokenizer.save_pretrained(MODEL_DIR)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in validation accuracy for {patience_counter} epoch(s).\")\n",
    "\n",
    "    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"Stopping early after {patience_counter} epochs with no improvement.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c08402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(f\"\\nTraining finished in {(end_time - start_time)/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0148ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history, save_dir=CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ef168b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(MODEL_DIR / \"test_save.txt\").write_text(\"This is a test file to check saving in models/distilbert-v4.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2643784d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".newvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
