{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9384976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ujjwalpoudel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ujjwalpoudel/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AdamW, get_scheduler, AutoConfig\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Importing the custom preprocessor\n",
    "sys.path.append(str(Path.cwd().parents[1] / \"src\"))\n",
    "from preprocessing.preprocess_dataset import DistilBertPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425307fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "sys.path.append(str(Path.cwd().parents[2]))\n",
    "DATA_DIR = Path(\"/Volumes/MACBACKUP/final_datasets/\")\n",
    "SAVE_PATH = Path(\"/models/distilber-v2\")\n",
    "CHECKPOINT_PATH = Path(\"./models/distilber-v2\")\n",
    "LEARNING_RATE = 3e-5\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 8\n",
    "EARLY_STOPPING_PATIENCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ebe49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPARATION\n",
    "#  PYTORCH DATASET CLASS\n",
    "class DepressionDataset(Dataset):\n",
    "    \"\"\"Custom Dataset class for our depression text data.\"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The encodings are already tensors from the tokenizer\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e69a7eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def evaluate(model, dataloader, device, loss_fn):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a given dataset.\n",
    "    NOW ACCEPTS a weighted loss_fn to calculate validation loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Get model outputs (logits)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # --- CHANGED ---\n",
    "            # Calculate loss manually using the provided weighted loss function\n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    # Use 'weighted' f1-score for a better view of imbalanced data\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') \n",
    "    \n",
    "    return avg_loss, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d78aa8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    \"\"\"Generates and saves plots for training metrics.\"\"\"\n",
    "    output_dir = Path(\"./plots\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    # Plots for Loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, history['train_loss'], 'b-o', label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], 'r-o', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(output_dir / 'distilbert-v2/training_validation_loss.png')\n",
    "    print(f\"Saved training and validation loss plot to {output_dir}\")\n",
    "\n",
    "    # Plots for Accuracy and F1-Score\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, history['train_accuracy'], 'c-o', label='Training Accuracy')\n",
    "    plt.plot(epochs, history['val_accuracy'], 'g-o', label='Validation Accuracy')\n",
    "    plt.plot(epochs, history['val_f1'], 'm-o', label='Validation F1-Score')\n",
    "    plt.title('Training/Validation Accuracy & F1-Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(output_dir / 'distilbert-v2/metrics.png')\n",
    "    print(f\"Saved metrics plot to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac11c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ujjwalpoudel/Documents/insane_projects/Conversational-Health-Analytics-/.newvenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (GPU) for training.\n",
      "Loading and preprocessing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2218 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded /Volumes/MACBACKUP/final_datasets/final_train_dataset.csv. Found 107 records.\n",
      "Text cleaning complete.\n",
      "Data prepared. X shape: (107, 1), y shape: (107, 1)\n",
      "\n",
      "Successfully loaded /Volumes/MACBACKUP/final_datasets/final_dev_dataset.csv. Found 35 records.\n",
      "Text cleaning complete.\n",
      "Data prepared. X shape: (35, 1), y shape: (35, 1)\n",
      "\n",
      "Starting chunking with strategy: 'sentence_aware'...\n",
      "Chunking complete. Original docs: 107, Total chunks: 407\n",
      "Starting chunking with strategy: 'sentence_aware'...\n",
      "Chunking complete. Original docs: 35, Total chunks: 147\n",
      "Tokenizing for DistilBERT with max_length=512...\n",
      "Tokenizing for DistilBERT with max_length=512...\n"
     ]
    }
   ],
   "source": [
    "# Device Setup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (GPU) for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Using CPU for training.\")\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "preprocessor = DistilBertPreprocessor()\n",
    "\n",
    "# Loading raw data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "X_train_df, y_train_df = preprocessor.load_and_preprocess(DATA_DIR / \"final_train_dataset.csv\")\n",
    "X_dev_df, y_dev_df = preprocessor.load_and_preprocess(DATA_DIR / \"final_dev_dataset.csv\")\n",
    "\n",
    "# Chunking the dataframes\n",
    "X_train_chunked, y_train_chunked = preprocessor.chunk_dataframe(X_train_df, y_train_df, strategy=\"sentence_aware\", chunk_size=510, overlap=2)\n",
    "X_dev_chunked, y_dev_chunked = preprocessor.chunk_dataframe(X_dev_df, y_dev_df, strategy=\"sentence_aware\", chunk_size=510, overlap=2)\n",
    "\n",
    "# Tokenizing the final chunked text\n",
    "train_encodings = preprocessor.tokenize(X_train_chunked['text'])\n",
    "dev_encodings = preprocessor.tokenize(X_dev_chunked['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c902c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INSPECTING CHUNK #50 ---\n",
      "\n",
      "ORIGINAL TEXT CHUNK:\n",
      "'t know i just don't get there i think that i'm a little bit afraid of losing my temper so i keep it under control laughter it was yesterday actually i argued with my boyfriend um about uh getting getting a job i was talking about taking a job somewhere and he was being very supportive and i really wanted him to to say no don't go take that job for six months'cause i would miss you and he was just being supportive so um we kind of argued about that laughter and it was ridiculous so we i ended up laughing and that was the end of the fight um meditate excercise go out in nature just um unplug get myself away from the computer away from technology away from people i would say not really um i'm an only child and it's just me and my parents and we're very very dissimilar for example um i'm a pretty adventurous person and i like learning and my parents are uh couch potatoes they like to watch reality television so we don't have too much to talk about or things that we can really do um as far as shared activites so you know i love them and everything but there's not too much common ground yes i think i'm a very social introvert but i seem to find most social encounters to be draining where as my my boyfriend for example he um derives a lot of pleasure and energy from being around people for me it's always a little bit the opposite i have to recharge i not sure i have a best friend but um i guess i'll say my boyfriend he's he's kinda my default best friend um he would probably say passionate intelligent um he always tells me that i'm really hard on myself which is true um dedicated loyal i think i'm just proud of who i am xxx i don't know if that's the answer that most people give but i think that i'm i'm really proud of who i am i try really hard um i've done a lot of things in my life i'm a really good person i think it's it's kind of rare to find people that are kind and i consider myself a kind person that that cares about others and wants to make the world a better place so i'm i'm proud of that laughter no i have not no yes i was probably about fifteen at the time so i think eight was is that seventeen years ago yes um i was cutting and my one of my'\n",
      "\n",
      "==================================================\n",
      "\n",
      "CONVERTED TOKENS (what the model sees):\n",
      "['[CLS]', 't', 'know', 'i', 'just', 'don', \"'\", 't', 'get', 'there', 'i', 'think', 'that', 'i', \"'\", 'm', 'a', 'little', 'bit', 'afraid', 'of', 'losing', 'my', 'temper', 'so', 'i', 'keep', 'it', 'under', 'control', 'laughter', 'it', 'was', 'yesterday', 'actually', 'i', 'argued', 'with', 'my', 'boyfriend', 'um', 'about', 'uh', 'getting', 'getting', 'a', 'job', 'i', 'was', 'talking', 'about', 'taking', 'a', 'job', 'somewhere', 'and', 'he', 'was', 'being', 'very', 'supportive', 'and', 'i', 'really', 'wanted', 'him', 'to', 'to', 'say', 'no', 'don', \"'\", 't', 'go', 'take', 'that', 'job', 'for', 'six', 'months', \"'\", 'cause', 'i', 'would', 'miss', 'you', 'and', 'he', 'was', 'just', 'being', 'supportive', 'so', 'um', 'we', 'kind', 'of', 'argued', 'about', 'that', 'laughter', 'and', 'it', 'was', 'ridiculous', 'so', 'we', 'i', 'ended', 'up', 'laughing', 'and', 'that', 'was', 'the', 'end', 'of', 'the', 'fight', 'um', 'med', '##itate', 'ex', '##cer', '##cise', 'go', 'out', 'in', 'nature', 'just', 'um', 'un', '##pl', '##ug', 'get', 'myself', 'away', 'from', 'the', 'computer', 'away', 'from', 'technology', 'away', 'from', 'people', 'i', 'would', 'say', 'not', 'really', 'um', 'i', \"'\", 'm', 'an', 'only', 'child', 'and', 'it', \"'\", 's', 'just', 'me', 'and', 'my', 'parents', 'and', 'we', \"'\", 're', 'very', 'very', 'di', '##ssi', '##mi', '##lar', 'for', 'example', 'um', 'i', \"'\", 'm', 'a', 'pretty', 'adventurous', 'person', 'and', 'i', 'like', 'learning', 'and', 'my', 'parents', 'are', 'uh', 'couch', 'potatoes', 'they', 'like', 'to', 'watch', 'reality', 'television', 'so', 'we', 'don', \"'\", 't', 'have', 'too', 'much', 'to', 'talk', 'about', 'or', 'things', 'that', 'we', 'can', 'really', 'do', 'um', 'as', 'far', 'as', 'shared', 'act', '##iv', '##ites', 'so', 'you', 'know', 'i', 'love', 'them', 'and', 'everything', 'but', 'there', \"'\", 's', 'not', 'too', 'much', 'common', 'ground', 'yes', 'i', 'think', 'i', \"'\", 'm', 'a', 'very', 'social', 'intro', '##vert', 'but', 'i', 'seem', 'to', 'find', 'most', 'social', 'encounters', 'to', 'be', 'draining', 'where', 'as', 'my', 'my', 'boyfriend', 'for', 'example', 'he', 'um', 'derives', 'a', 'lot', 'of', 'pleasure', 'and', 'energy', 'from', 'being', 'around', 'people', 'for', 'me', 'it', \"'\", 's', 'always', 'a', 'little', 'bit', 'the', 'opposite', 'i', 'have', 'to', 'rec', '##har', '##ge', 'i', 'not', 'sure', 'i', 'have', 'a', 'best', 'friend', 'but', 'um', 'i', 'guess', 'i', \"'\", 'll', 'say', 'my', 'boyfriend', 'he', \"'\", 's', 'he', \"'\", 's', 'kinda', 'my', 'default', 'best', 'friend', 'um', 'he', 'would', 'probably', 'say', 'passionate', 'intelligent', 'um', 'he', 'always', 'tells', 'me', 'that', 'i', \"'\", 'm', 'really', 'hard', 'on', 'myself', 'which', 'is', 'true', 'um', 'dedicated', 'loyal', 'i', 'think', 'i', \"'\", 'm', 'just', 'proud', 'of', 'who', 'i', 'am', 'xx', '##x', 'i', 'don', \"'\", 't', 'know', 'if', 'that', \"'\", 's', 'the', 'answer', 'that', 'most', 'people', 'give', 'but', 'i', 'think', 'that', 'i', \"'\", 'm', 'i', \"'\", 'm', 'really', 'proud', 'of', 'who', 'i', 'am', 'i', 'try', 'really', 'hard', 'um', 'i', \"'\", 've', 'done', 'a', 'lot', 'of', 'things', 'in', 'my', 'life', 'i', \"'\", 'm', 'a', 'really', 'good', 'person', 'i', 'think', 'it', \"'\", 's', 'it', \"'\", 's', 'kind', 'of', 'rare', 'to', 'find', 'people', 'that', 'are', 'kind', 'and', 'i', 'consider', 'myself', 'a', 'kind', 'person', 'that', 'that', 'cares', 'about', 'others', 'and', 'wants', 'to', 'make', 'the', 'world', 'a', 'better', 'place', 'so', 'i', \"'\", 'm', 'i', \"'\", 'm', 'proud', 'of', 'that', 'laughter', 'no', 'i', 'have', 'not', 'no', 'yes', 'i', 'was', 'probably', 'about', 'fifteen', 'at', 'the', 'time', 'so', 'i', 'think', 'eight', 'was', 'is', 'that', 'seventeen', 'years', 'ago', 'yes', 'um', 'i', 'was', 'cutting', 'and', 'my', 'one', 'of', 'my', '[SEP]']\n",
      "\n",
      "==================================================\n",
      "\n",
      "INPUT IDS (numerical representation of tokens):\n",
      "[  101  1056  2113  1045  2074  2123  1005  1056  2131  2045  1045  2228\n",
      "  2008  1045  1005  1049  1037  2210  2978  4452  1997  3974  2026 12178\n",
      "  2061  1045  2562  2009  2104  2491  7239  2009  2001  7483  2941  1045\n",
      "  5275  2007  2026  6898  8529  2055  7910  2893  2893  1037  3105  1045\n",
      "  2001  3331  2055  2635  1037  3105  4873  1998  2002  2001  2108  2200\n",
      " 16408  1998  1045  2428  2359  2032  2000  2000  2360  2053  2123  1005\n",
      "  1056  2175  2202  2008  3105  2005  2416  2706  1005  3426  1045  2052\n",
      "  3335  2017  1998  2002  2001  2074  2108 16408  2061  8529  2057  2785\n",
      "  1997  5275  2055  2008  7239  1998  2009  2001  9951  2061  2057  1045\n",
      "  3092  2039  5870  1998  2008  2001  1996  2203  1997  1996  2954  8529\n",
      " 19960 17570  4654 17119 18380  2175  2041  1999  3267  2074  8529  4895\n",
      " 24759 15916  2131  2870  2185  2013  1996  3274  2185  2013  2974  2185\n",
      "  2013  2111  1045  2052  2360  2025  2428  8529  1045  1005  1049  2019\n",
      "  2069  2775  1998  2009  1005  1055  2074  2033  1998  2026  3008  1998\n",
      "  2057  1005  2128  2200  2200  4487 18719  4328  8017  2005  2742  8529\n",
      "  1045  1005  1049  1037  3492 29118  2711  1998  1045  2066  4083  1998\n",
      "  2026  3008  2024  7910  6411 14629  2027  2066  2000  3422  4507  2547\n",
      "  2061  2057  2123  1005  1056  2031  2205  2172  2000  2831  2055  2030\n",
      "  2477  2008  2057  2064  2428  2079  8529  2004  2521  2004  4207  2552\n",
      " 12848  7616  2061  2017  2113  1045  2293  2068  1998  2673  2021  2045\n",
      "  1005  1055  2025  2205  2172  2691  2598  2748  1045  2228  1045  1005\n",
      "  1049  1037  2200  2591 17174 16874  2021  1045  4025  2000  2424  2087\n",
      "  2591 11340  2000  2022 19689  2073  2004  2026  2026  6898  2005  2742\n",
      "  2002  8529 12153  1037  2843  1997  5165  1998  2943  2013  2108  2105\n",
      "  2111  2005  2033  2009  1005  1055  2467  1037  2210  2978  1996  4500\n",
      "  1045  2031  2000 28667  8167  3351  1045  2025  2469  1045  2031  1037\n",
      "  2190  2767  2021  8529  1045  3984  1045  1005  2222  2360  2026  6898\n",
      "  2002  1005  1055  2002  1005  1055 17704  2026 12398  2190  2767  8529\n",
      "  2002  2052  2763  2360 13459  9414  8529  2002  2467  4136  2033  2008\n",
      "  1045  1005  1049  2428  2524  2006  2870  2029  2003  2995  8529  4056\n",
      "  8884  1045  2228  1045  1005  1049  2074  7098  1997  2040  1045  2572\n",
      " 22038  2595  1045  2123  1005  1056  2113  2065  2008  1005  1055  1996\n",
      "  3437  2008  2087  2111  2507  2021  1045  2228  2008  1045  1005  1049\n",
      "  1045  1005  1049  2428  7098  1997  2040  1045  2572  1045  3046  2428\n",
      "  2524  8529  1045  1005  2310  2589  1037  2843  1997  2477  1999  2026\n",
      "  2166  1045  1005  1049  1037  2428  2204  2711  1045  2228  2009  1005\n",
      "  1055  2009  1005  1055  2785  1997  4678  2000  2424  2111  2008  2024\n",
      "  2785  1998  1045  5136  2870  1037  2785  2711  2008  2008 14977  2055\n",
      "  2500  1998  4122  2000  2191  1996  2088  1037  2488  2173  2061  1045\n",
      "  1005  1049  1045  1005  1049  7098  1997  2008  7239  2053  1045  2031\n",
      "  2025  2053  2748  1045  2001  2763  2055  5417  2012  1996  2051  2061\n",
      "  1045  2228  2809  2001  2003  2008  9171  2086  3283  2748  8529  1045\n",
      "  2001  6276  1998  2026  2028  1997  2026   102]\n",
      "\n",
      "==================================================\n",
      "\n",
      "ATTENTION MASK (1s are real tokens, 0s are padding):\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "==================================================\n",
      "Total Token Length: 512\n"
     ]
    }
   ],
   "source": [
    "# We are using the 'preprocessor' object which contains the tokenizer\n",
    "tokenizer = preprocessor.tokenizer\n",
    "\n",
    "# Choose a chunk to inspect (e.g., the first chunk: index 0)\n",
    "chunk_index = 50\n",
    "\n",
    "# Getting the original text for this chunk\n",
    "original_text_chunk = X_train_chunked['text'].iloc[chunk_index]\n",
    "\n",
    "# Get the tokenized outputs for this chunk\n",
    "input_ids_tensor = train_encodings['input_ids'][chunk_index]\n",
    "attention_mask_tensor = train_encodings['attention_mask'][chunk_index]\n",
    "\n",
    "# Convert the tensor of token IDs back into a list of human-readable tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids_tensor)\n",
    "\n",
    "# --- Print the results for verification ---\n",
    "print(f\"--- INSPECTING CHUNK #{chunk_index} ---\")\n",
    "print(f\"\\nORIGINAL TEXT CHUNK:\\n'{original_text_chunk}'\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"\\nCONVERTED TOKENS (what the model sees):\\n{tokens}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"\\nINPUT IDS (numerical representation of tokens):\\n{input_ids_tensor.numpy()}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"\\nATTENTION MASK (1s are real tokens, 0s are padding):\\n{attention_mask_tensor.numpy()}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Total Token Length: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d625c72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating PyTorch Datasets\n",
    "train_dataset = DepressionDataset(train_encodings, y_train_chunked['label'].tolist())\n",
    "dev_dataset = DepressionDataset(dev_encodings, y_dev_chunked['label'].tolist())\n",
    "\n",
    "# Creating DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a497a79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Class Weights to fix imbalance: tensor([0.2776, 0.7224], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "label_counts = y_train_chunked['label'].value_counts().sort_index()\n",
    "weights = 1.0 / torch.tensor(label_counts.values, dtype=torch.float)\n",
    "class_weights = (weights / weights.sum()).to(device)\n",
    "print(f\"Using Class Weights to fix imbalance: {class_weights}\")\n",
    "    \n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "863b7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_distilbert_classifier(num_labels=2):\n",
    "    \"\"\"\n",
    "    Builds and compiles the DistilBERT model.\n",
    "    \"\"\"\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=num_labels,\n",
    "        attention_dropout=0.3,\n",
    "        dropout=0.2\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ba09c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model, optimizer, and scheduler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ujjwalpoudel/Documents/insane_projects/Conversational-Health-Analytics-/.newvenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ujjwalpoudel/Documents/insane_projects/Conversational-Health-Analytics-/.newvenv/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Setting up model, optimizer, and scheduler...\")\n",
    "model, optimizer = build_distilbert_classifier(num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f92037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = EPOCHS * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e87465b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "    \n",
    "history = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': [], 'val_f1': []}\n",
    "best_val_accuracy = 0.0\n",
    "patience_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1f507c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/51 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on development set...\n",
      "Epoch 1 | Train Loss: 0.6955 | Train Acc: 0.5405 | Val Loss: 0.7031 | Val Acc: 0.5986 | Val F1: 0.4483\n",
      "Validation accuracy improved from 0.0000 to 0.5986. Saving model...\n",
      "\n",
      "--- Epoch 2/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on development set...\n",
      "Epoch 2 | Train Loss: 0.6882 | Train Acc: 0.6978 | Val Loss: 0.6765 | Val Acc: 0.6395 | Val F1: 0.5581\n",
      "Validation accuracy improved from 0.5986 to 0.6395. Saving model...\n",
      "\n",
      "--- Epoch 3/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on development set...\n",
      "Epoch 3 | Train Loss: 0.6556 | Train Acc: 0.6806 | Val Loss: 0.6831 | Val Acc: 0.5578 | Val F1: 0.5503\n",
      "No improvement in validation accuracy for 1 epoch(s).\n",
      "\n",
      "--- Epoch 4/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on development set...\n",
      "Epoch 4 | Train Loss: 0.5728 | Train Acc: 0.7371 | Val Loss: 0.7699 | Val Acc: 0.5238 | Val F1: 0.5034\n",
      "No improvement in validation accuracy for 2 epoch(s).\n",
      "\n",
      "--- Epoch 5/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on development set...\n",
      "Epoch 5 | Train Loss: 0.4133 | Train Acc: 0.8305 | Val Loss: 0.7991 | Val Acc: 0.6463 | Val F1: 0.6254\n",
      "Validation accuracy improved from 0.6395 to 0.6463. Saving model...\n",
      "\n",
      "--- Epoch 6/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on development set...\n",
      "Epoch 6 | Train Loss: 0.2087 | Train Acc: 0.9410 | Val Loss: 1.0338 | Val Acc: 0.6190 | Val F1: 0.6190\n",
      "No improvement in validation accuracy for 1 epoch(s).\n",
      "\n",
      "--- Epoch 7/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on development set...\n",
      "Epoch 7 | Train Loss: 0.1586 | Train Acc: 0.9558 | Val Loss: 1.0710 | Val Acc: 0.6259 | Val F1: 0.6212\n",
      "No improvement in validation accuracy for 2 epoch(s).\n",
      "\n",
      "--- Epoch 8/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on development set...\n",
      "Epoch 8 | Train Loss: 0.0926 | Train Acc: 0.9730 | Val Loss: 1.1419 | Val Acc: 0.6327 | Val F1: 0.6215\n",
      "No improvement in validation accuracy for 3 epoch(s).\n",
      "Stopping early after 3 epochs with no improvement.\n",
      "\n",
      "Training finished in 28.75 minutes.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "plot_metrics() got an unexpected keyword argument 'save_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining finished in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time)/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutes.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Generate and save plots\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43mplot_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: plot_metrics() got an unexpected keyword argument 'save_dir'"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch + 1}/{EPOCHS} ---\")\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        epoch_preds = []\n",
    "        epoch_labels = []\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            epoch_preds.extend(preds.cpu().numpy())\n",
    "            epoch_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        progress_bar.close()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = accuracy_score(epoch_labels, epoch_preds)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_accuracy'].append(train_accuracy)\n",
    "\n",
    "        print(\"Evaluating on development set...\")\n",
    "        val_loss, val_accuracy, val_f1 = evaluate(model, dev_loader, device, loss_fn)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            print(f\"Validation accuracy improved from {best_val_accuracy:.4f} to {val_accuracy:.4f}. Saving model...\")\n",
    "            best_val_accuracy = val_accuracy\n",
    "            patience_counter = 0\n",
    "            \n",
    "            CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "            model.save_pretrained(CHECKPOINT_PATH)\n",
    "            preprocessor.tokenizer.save_pretrained(CHECKPOINT_PATH)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation accuracy for {patience_counter} epoch(s).\")\n",
    "        \n",
    "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"Stopping early after {patience_counter} epochs with no improvement.\")\n",
    "            break\n",
    "        \n",
    "end_time = time.time()\n",
    "print(f\"\\nTraining finished in {(end_time - start_time)/60:.2f} minutes.\")\n",
    "    \n",
    "# Generate and save plots\n",
    "plot_metrics(history, save_dir=CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44983dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c5f61b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7075b13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".newvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
